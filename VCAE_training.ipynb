{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1eZt5YEQ432h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from skimage import io\n",
        "from skimage.filters import gaussian\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w4iY1JtB5BeS"
      },
      "outputs": [],
      "source": [
        "# Define CVAE and functions needed\n",
        "class CVAE(nn.Module):\n",
        "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim, size):\n",
        "        super(CVAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.size = size\n",
        "        reduced_size = size // 8\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * reduced_size * reduced_size, latent_dim * 2)\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, reduced_size * reduced_size * 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (32, reduced_size, reduced_size)),\n",
        "            nn.ConvTranspose2d(32, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "        logits = self.decoder(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = torch.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        return self.decode(z)\n",
        "\n",
        "def generate_images(model, epoch, test_sample):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        mean, logvar = model.encode(test_sample)\n",
        "        z = model.reparameterize(mean, logvar)\n",
        "        predictions = model.decode(z, apply_sigmoid=True)\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(4, 4))\n",
        "    for i, ax in enumerate(axs.flatten()):\n",
        "        ax.imshow(predictions[i, 0, :, :].cpu().numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(4, 4))\n",
        "    for i, ax in enumerate(axs.flatten()):\n",
        "        ax.imshow(test_sample[i, 0, :, :].cpu().numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def compute_loss(model, x):\n",
        "    mean, logvar = model.encode(x)\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.decode(z)\n",
        "    cross_ent = F.binary_cross_entropy_with_logits(x_logit, x, reduction='sum')\n",
        "    logpx_z = -cross_ent\n",
        "    logpz = log_normal_pdf(z, torch.zeros_like(z), torch.zeros_like(z))\n",
        "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "    return -torch.mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar):\n",
        "    log2pi = torch.log(torch.tensor(2. * np.pi))\n",
        "    return torch.sum(-0.5 * ((sample - mean) ** 2 * torch.exp(-logvar) + logvar + log2pi), dim=1)\n",
        "\n",
        "def train_step(model, x, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss = compute_loss(model, x)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def predict(model, inp_image, apply_sigmoid=True):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        mean, logvar = model.encode(inp_image)\n",
        "        z = model.reparameterize(mean, logvar)\n",
        "        predictions = model.decode(z, apply_sigmoid)\n",
        "    return predictions[0, 0, :, :].cpu().numpy()\n",
        "\n",
        "def get_testing_training_sets(image='20111206DF', size=64, n_images=10):\n",
        "    training = []\n",
        "    testing = []\n",
        "    img = io.imread(image)\n",
        "    width = len(img[0])\n",
        "    height = len(img)\n",
        "    try:\n",
        "        img = img[:, :, 0]\n",
        "    except:\n",
        "        pass\n",
        "    for i in range(n_images):\n",
        "        shift_y = np.random.randint(height // 2, height - size)\n",
        "        shift_x = np.random.randint(0, width - size)\n",
        "        training.append(img[shift_y:shift_y + size, shift_x:shift_x + size])\n",
        "        shift_y = np.random.randint(0, height // 2 - size)\n",
        "        shift_x = np.random.randint(0, width - size)\n",
        "        testing.append(img[shift_y:shift_y + size, shift_x:shift_x + size])\n",
        "    return np.array(training), np.array(testing)\n",
        "\n",
        "def gaussian_blur(img, sigma):\n",
        "    return np.array(gaussian(img, (sigma, sigma)))\n",
        "\n",
        "def gaussian_blur_arr(images, sigma):\n",
        "    return np.array([gaussian_blur(img, sigma) for img in images])\n",
        "\n",
        "def norm_max_pixel(images):\n",
        "    return np.array([img / np.max(img) for img in images])\n",
        "\n",
        "def preprocess_images(images, size, sigma):\n",
        "    images = gaussian_blur_arr(images, sigma)\n",
        "    images = norm_max_pixel(images)\n",
        "    images = images.reshape((images.shape[0], 1, size, size))\n",
        "    return images.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b9mZdLhI7rpG"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataraw_training_set_20111206DF_64.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m training_set, testing_set \u001b[38;5;241m=\u001b[39m get_testing_training_sets(path \u001b[38;5;241m+\u001b[39m image_file, SIZE, \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m mix_training_set \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw_training_set_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_64.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(mix_training_set)\n\u001b[1;32m     12\u001b[0m testing_set \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_testing_set_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m image_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_64.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m100\u001b[39m]\n",
            "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ml-local-cM0SADfM-py3.12/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataraw_training_set_20111206DF_64.npy'"
          ]
        }
      ],
      "source": [
        "SIZE = 64 \n",
        "SIGMA = 2\n",
        "epochs = 200\n",
        "latent_dim = 20\n",
        "image_file = \"20111206DF.jpg\"\n",
        "path = \"\"\n",
        "\n",
        "# Load and preprocess data\n",
        "training_set, testing_set = get_testing_training_sets(path + image_file, SIZE, 1000)\n",
        "mix_training_set = np.load(path + 'data' + \"raw_training_set_\" + image_file.split('.')[0] + \"_64.npy\")\n",
        "np.random.shuffle(mix_training_set)\n",
        "testing_set = np.load(path + \"raw_testing_set_\" + image_file.split('.')[0] + \"_64.npy\")[:100]\n",
        "\n",
        "train_images = preprocess_images(training_set, SIZE, SIGMA)\n",
        "test_images = preprocess_images(testing_set, SIZE, SIGMA)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(torch.tensor(train_images))\n",
        "test_dataset = TensorDataset(torch.tensor(test_images))\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Instantiate model and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CVAE(latent_dim, SIZE).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_examples_to_generate = 16\n",
        "random_vector_for_generation = torch.randn(num_examples_to_generate, latent_dim).to(device)\n",
        "\n",
        "# Save raw testing and training sets\n",
        "np.save(path + \"raw_training_set_\" + image_file.split('.')[0] + \"_64.npy\", training_set)\n",
        "np.save(path + \"raw_testing_set_\" + image_file.split('.')[0] + \"_64.npy\", testing_set)\n",
        "\n",
        "# Pick a sample of the test set for generating output images\n",
        "test_sample = next(iter(test_loader))[0][:num_examples_to_generate].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBVb32Lz27d4"
      },
      "outputs": [],
      "source": [
        "generate_images(model, 0, test_sample)\n",
        "try: \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            train_x = batch[0].to(device)\n",
        "            loss = compute_loss(model, train_x)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        end_time = time.time()\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                test_x = batch[0].to(device)\n",
        "                test_loss += compute_loss(model, test_x).item()\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "        display.clear_output(wait=False)\n",
        "        print(f'Epoch: {epoch}, Test set ELBO: {test_loss:.4f}, time elapsed for current epoch: {end_time - start_time:.2f}s')\n",
        "        generate_images(model, epoch, test_sample)\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"Training interrupted. Saving the model...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), path + image_file.strip(\".jpg\") + f'_Size{SIZE}_SIGMA{SIGMA}_epochs{epochs}_latentdim{latent_dim}.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNMaeCCdzuYk/885RBozhHe",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
