{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "1eZt5YEQ432h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from skimage import io\n",
        "from skimage.filters import gaussian\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import hyperspy.api as hs\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "w4iY1JtB5BeS"
      },
      "outputs": [],
      "source": [
        "class CVAE3D(nn.Module):\n",
        "    \"\"\"Convolutional variational autoencoder.\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim, size):\n",
        "        super(CVAE3D, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.size = size\n",
        "        reduced_size = int(size/8)\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(1, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * reduced_size * reduced_size * reduced_size, latent_dim * 2)\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, reduced_size * reduced_size * reduced_size * 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (32, reduced_size, reduced_size, reduced_size)),\n",
        "            nn.ConvTranspose3d(32, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose3d(64, 1, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mean, logvar = torch.chunk(h, 2, dim=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mean + eps * std\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "        logits = self.decoder(z)\n",
        "        if apply_sigmoid:\n",
        "            probs = torch.sigmoid(logits)\n",
        "            return probs\n",
        "        return logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        return self.decode(z)\n",
        "       \n",
        "############################################################################################################\n",
        "##################################### END OF MODEL DEFINITION ##############################################\n",
        "############################################################################################################\n",
        "\n",
        "def load_dm4_data(filepath):\n",
        "    s = hs.load(filepath)\n",
        "    data = s.data  # The 3D data array\n",
        "    return data\n",
        "\n",
        "def preprocess_3d_images(images, size, sigma):\n",
        "    # Check if input is a single 3D image and convert it to a batch\n",
        "    if images.ndim == 3:\n",
        "        images = np.expand_dims(images, axis=0)\n",
        "    \n",
        "    # Apply Gaussian blur to each 3D image\n",
        "    blurred_images = np.array([gaussian(img, sigma=sigma) for img in images])\n",
        "    \n",
        "    # Normalize the images\n",
        "    normalized_images = blurred_images / np.max(blurred_images)\n",
        "    \n",
        "    reshaped_images = []\n",
        "    for img in normalized_images:\n",
        "        # Calculate the padding for each dimension\n",
        "        padding = [(max(0, size - dim_size) // 2, max(0, size - dim_size) - max(0, size - dim_size) // 2) for dim_size in img.shape]\n",
        "        \n",
        "        # Apply padding\n",
        "        padded_img = np.pad(img, padding, mode='constant')\n",
        "        \n",
        "        # Calculate the crop for each dimension\n",
        "        crop = [(max(0, padded_img.shape[i] - size) // 2, max(0, padded_img.shape[i] - size) - max(0, padded_img.shape[i] - size) // 2) for i in range(len(padded_img.shape))]\n",
        "        cropped_img = padded_img[crop[0][0]:padded_img.shape[0]-crop[0][1],\n",
        "                                 crop[1][0]:padded_img.shape[1]-crop[1][1],\n",
        "                                 crop[2][0]:padded_img.shape[2]-crop[2][1]]\n",
        "        \n",
        "        # Ensure the final shape matches the target size\n",
        "        assert cropped_img.shape == (size, size, size), f\"Shape mismatch: {cropped_img.shape} != {(size, size, size)}\"\n",
        "        \n",
        "        # Append the reshaped image\n",
        "        reshaped_images.append(cropped_img)\n",
        "    \n",
        "    reshaped_images = np.array(reshaped_images)\n",
        "    \n",
        "    # Reshape to (N, 1, size, size, size) for PyTorch\n",
        "    reshaped_images = reshaped_images.reshape((reshaped_images.shape[0], 1, size, size, size))\n",
        "    \n",
        "    return reshaped_images.astype('float32')\n",
        "\n",
        "\n",
        "############################################################################################################\n",
        "##################################### END OF DATA PREPROCESSING ############################################\n",
        "############################################################################################################\n",
        "\n",
        "def generate_images(model, epoch, test_sample):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        mean, logvar = model.encode(test_sample)\n",
        "        z = model.reparameterize(mean, logvar)\n",
        "        predictions = model.decode(z, apply_sigmoid=True)\n",
        "\n",
        "    num_images = predictions.shape[0]\n",
        "    num_rows = int(np.ceil(np.sqrt(num_images)))\n",
        "    num_cols = int(np.ceil(num_images / num_rows))\n",
        "\n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(5, 5))\n",
        "    if num_rows * num_cols == 1:\n",
        "        axs = np.array([axs])\n",
        "    else:\n",
        "        axs = axs.flatten()\n",
        "    for i, ax in enumerate(axs):\n",
        "        if i >= predictions.shape[0]:\n",
        "            ax.axis('off')\n",
        "            continue\n",
        "        # Visualize the middle slice of the 3D prediction\n",
        "        middle_slice = predictions[i, 0, :, :, predictions.shape[2] // 2].cpu().numpy()\n",
        "        ax.imshow(middle_slice, cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(5, 5))\n",
        "    if num_rows * num_cols == 1:\n",
        "        axs = np.array([axs])\n",
        "    else:\n",
        "        axs = axs.flatten()\n",
        "    for i, ax in enumerate(axs):\n",
        "        if i >= test_sample.shape[0]:\n",
        "            ax.axis('off')\n",
        "            continue\n",
        "        # Visualize the middle slice of the 3D input sample\n",
        "        middle_slice = test_sample[i, 0, :, :, test_sample.shape[2] // 2].cpu().numpy()\n",
        "        ax.imshow(middle_slice, cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "def compute_loss(model, x):\n",
        "    mean, logvar = model.encode(x)\n",
        "    z = model.reparameterize(mean, logvar)\n",
        "    x_logit = model.decode(z)\n",
        "    cross_ent = F.binary_cross_entropy_with_logits(x_logit, x, reduction='sum')\n",
        "    logpx_z = -cross_ent\n",
        "    logpz = log_normal_pdf(z, torch.zeros_like(z), torch.zeros_like(z))\n",
        "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "    return -torch.mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar):\n",
        "    log2pi = torch.log(torch.tensor(2. * np.pi))\n",
        "    return torch.sum(-0.5 * ((sample - mean) ** 2 * torch.exp(-logvar) + logvar + log2pi), dim=1)\n",
        "\n",
        "def train_step(model, x, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss = compute_loss(model, x)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def predict(model, inp_image, apply_sigmoid=True):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        mean, logvar = model.encode(inp_image)\n",
        "        z = model.reparameterize(mean, logvar)\n",
        "        predictions = model.decode(z, apply_sigmoid)\n",
        "    return predictions[0, 0, :, :, predictions.shape[2] // 2].cpu().numpy()\n",
        "\n",
        "def get_testing_training_sets(image, size=64, n_images=10):\n",
        "    training = []\n",
        "    testing = []\n",
        "    img = io.imread(image)\n",
        "    width = len(img[0])\n",
        "    height = len(img)\n",
        "    try:\n",
        "        img = img[:, :, 0]\n",
        "    except:\n",
        "        pass\n",
        "    for i in range(n_images):\n",
        "        shift_y = np.random.randint(height // 2, height - size)\n",
        "        shift_x = np.random.randint(0, width - size)\n",
        "        training.append(img[shift_y:shift_y + size, shift_x:shift_x + size])\n",
        "        shift_y = np.random.randint(0, height // 2 - size)\n",
        "        shift_x = np.random.randint(0, width - size)\n",
        "        testing.append(img[shift_y:shift_y + size, shift_x:shift_x + size])\n",
        "    return np.array(training), np.array(testing)\n",
        "\n",
        "def gaussian_blur(img, sigma):\n",
        "    return np.array(gaussian(img, (sigma, sigma)))\n",
        "\n",
        "def gaussian_blur_arr(images, sigma):\n",
        "    return np.array([gaussian_blur(img, sigma) for img in images])\n",
        "\n",
        "def norm_max_pixel(images):\n",
        "    return np.array([img / np.max(img) for img in images])\n",
        "\n",
        "def preprocess_images(images, size, sigma):\n",
        "    images = gaussian_blur_arr(images, sigma)\n",
        "    images = norm_max_pixel(images)\n",
        "    images = images.reshape((images.shape[0], 1, size, size))\n",
        "    return images.astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(51, 50, 100)\n"
          ]
        }
      ],
      "source": [
        "SIZE = 48\n",
        "SIGMA = 2\n",
        "epochs = 400\n",
        "latent_dim = 20\n",
        "dm4_file = 'data/images_3D/EELS HL SI.dm4'\n",
        "path = \"\"\n",
        "\n",
        "# Load the data with the specified energy range and resolution\n",
        "data = load_dm4_data(dm4_file)\n",
        "\n",
        "# Specify the energy range and resolution\n",
        "energy_range = (500, 600)  # eV\n",
        "ev_per_pixel = 0.05\n",
        "\n",
        "# Calculate the pixel indices corresponding to the energy range\n",
        "start_pixel = int((energy_range[0] - 0))\n",
        "end_pixel = int((energy_range[1] - 0))\n",
        "\n",
        "# Slice the data array to keep only the desired energy range in the third dimension\n",
        "data = data[:, :, start_pixel:end_pixel]\n",
        "\n",
        "# show the shape of the data \n",
        "print(data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "b9mZdLhI7rpG"
      },
      "outputs": [],
      "source": [
        "train_images = preprocess_3d_images(data, size=SIZE, sigma=SIGMA)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(torch.tensor(train_images))\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Instantiate model and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CVAE3D(latent_dim, SIZE).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Pick a sample of the test set for generating output images\n",
        "num_examples_to_generate = 1\n",
        "test_sample = next(iter(train_loader))[0][:num_examples_to_generate].to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "IBVb32Lz27d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 193, Test set ELBO: 65972.9844, time elapsed for current epoch: 0.20s\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training interrupted. Saving the model...\n"
          ]
        }
      ],
      "source": [
        "# Generate initial images\n",
        "generate_images(model, 0, test_sample)\n",
        "\n",
        "# Training loop\n",
        "try: \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            train_x = batch[0].to(device)\n",
        "            loss = compute_loss(model, train_x)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        end_time = time.time()\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in train_loader:\n",
        "                test_x = batch[0].to(device)\n",
        "                test_loss += compute_loss(model, test_x).item()\n",
        "        test_loss /= len(train_loader.dataset)\n",
        "    \n",
        "        display.clear_output(wait=False)\n",
        "        print(f'Epoch: {epoch}, Test set ELBO: {test_loss:.4f}, time elapsed for current epoch: {end_time - start_time:.2f}s')\n",
        "        generate_images(model, epoch, test_sample)\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"Training interrupted. Saving the model...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), path + 'cvae3d.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNMaeCCdzuYk/885RBozhHe",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
